{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------INPUT TEXT-------------\n",
      "\n",
      "The capital city of Ethiopia is Addis Abeba \n",
      "\n",
      "\n",
      "-----------INPUT END---------------\n",
      "\n",
      "bucket=\n",
      "{'DT': 0}\n",
      "TAGS: [('The', 'DT'), ('capital', 'NN'), ('city', 'NN'), ('of', 'IN'), ('Ethiopia', 'NNP'), ('is', 'VBZ'), ('Addis', 'NNP'), ('Abeba', 'NNP')] \n",
      "\n",
      "bucket=\n",
      "{'DT': 0, 'NN': 1}\n",
      "TAGS: [('The', 'DT'), ('capital', 'NN'), ('city', 'NN'), ('of', 'IN'), ('Ethiopia', 'NNP'), ('is', 'VBZ'), ('Addis', 'NNP'), ('Abeba', 'NNP')] \n",
      "\n",
      "bucket=\n",
      "{'DT': 0, 'NN': 1, 'IN': 3}\n",
      "TAGS: [('The', 'DT'), ('capital', 'NN'), ('city', 'NN'), ('of', 'IN'), ('Ethiopia', 'NNP'), ('is', 'VBZ'), ('Addis', 'NNP'), ('Abeba', 'NNP')] \n",
      "\n",
      "bucket=\n",
      "{'DT': 0, 'NN': 1, 'IN': 3, 'NNP': 4}\n",
      "TAGS: [('The', 'DT'), ('capital', 'NN'), ('city', 'NN'), ('of', 'IN'), ('Ethiopia', 'NNP'), ('is', 'VBZ'), ('Addis', 'NNP'), ('Abeba', 'NNP')] \n",
      "\n",
      "bucket=\n",
      "{'DT': 0, 'NN': 1, 'IN': 3, 'NNP': 4, 'VBZ': 5}\n",
      "TAGS: [('The', 'DT'), ('capital', 'NN'), ('city', 'NN'), ('of', 'IN'), ('Ethiopia', 'NNP'), ('is', 'VBZ'), ('Addis', 'NNP'), ('Abeba', 'NNP')] \n",
      "\n",
      "\n",
      " Question: What is Ethiopia?\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from textblob import Word\n",
    "import sys\n",
    "\n",
    "\n",
    "def parse(string):\n",
    "    \"\"\"\n",
    "    Parse a paragraph. Devide it into sentences and try to generate questions from each sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        txt = TextBlob(string)\n",
    "        \n",
    "        # Each sentence is taken from the string input and passed to genQuestion() to generate questions.\n",
    "        for sentence in txt.sentences:\n",
    "            genQuestion(sentence)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "def genQuestion(line):\n",
    "    \"\"\"\n",
    "    outputs question from the given text\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if type(line) is str:     # If the passed variable is of type string.\n",
    "        line = TextBlob(line) # Create object of type textblob.blob.TextBlob\n",
    "\n",
    "    bucket = {}               # Create an empty dictionary\n",
    "\n",
    "\n",
    "    for i,j in enumerate(line.tags):  # line.tags are the parts-of-speach in English\n",
    "        if j[1] not in bucket:\n",
    "            bucket[j[1]] = i  # Add all tags to the dictionary or bucket variable\n",
    "            print(\"bucket=\") \n",
    "            print(bucket)\n",
    "            print(\"TAGS:\",line.tags, '\\n')  \n",
    "    \n",
    "    if verbose:               # In verbose more print the key,values of dictionary\n",
    "        print('\\n','-'*20)\n",
    "        print(line ,'\\n')        \n",
    "        print(\"TAGS:\",line.tags, '\\n')  \n",
    "        print(bucket)\n",
    "    \n",
    "    question = ''            # Create an empty string \n",
    "\n",
    "    # These are the english part-of-speach tags used in this demo program.\n",
    "    #.....................................................................\n",
    "    # NNS     Noun, plural\n",
    "    # JJ  Adjective \n",
    "    # NNP     Proper noun, singular \n",
    "    # VBG     Verb, gerund or present participle \n",
    "    # VBN     Verb, past participle \n",
    "    # VBZ     Verb, 3rd person singular present \n",
    "    # VBD     Verb, past tense \n",
    "    # IN      Preposition or subordinating conjunction \n",
    "    # PRP     Personal pronoun \n",
    "    # NN  Noun, singular or mass \n",
    "    #.....................................................................\n",
    "\n",
    "    # Create a list of tag-combination\n",
    "\n",
    "    l1 = ['NNP', 'VBG', 'VBZ', 'IN']\n",
    "    l2 = ['NNP', 'VBG', 'VBZ']\n",
    "    \n",
    "\n",
    "    l3 = ['PRP', 'VBG', 'VBZ', 'IN']\n",
    "    l4 = ['PRP', 'VBG', 'VBZ']\n",
    "    l5 = ['PRP', 'VBG', 'VBD']\n",
    "    l6 = ['NNP', 'VBG', 'VBD']\n",
    "    l7 = ['NN', 'VBG', 'VBZ']\n",
    "\n",
    "    l8 = ['NNP', 'VBZ', 'JJ']\n",
    "    l9 = ['NNP', 'VBZ', 'NN']\n",
    "\n",
    "    l10 = ['NNP', 'VBZ']\n",
    "    l11 = ['PRP', 'VBZ']\n",
    "    l12 = ['NNP', 'NN', 'IN']\n",
    "    l13 = ['NN', 'VBZ']\n",
    "\n",
    "\n",
    "    # With the use of conditional statements the dictionary is compared with the list created above\n",
    "\n",
    "    \n",
    "    if all(key in  bucket for key in l1): #'NNP', 'VBG', 'VBZ', 'IN' in sentence.\n",
    "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']]+ ' '+ line.words[bucket['VBG']] + '?'\n",
    "\n",
    "    \n",
    "    elif all(key in  bucket for key in l2): #'NNP', 'VBG', 'VBZ' in sentence.\n",
    "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']] +' '+ line.words[bucket['VBG']] + '?'\n",
    "\n",
    "    \n",
    "    elif all(key in  bucket for key in l3): #'PRP', 'VBG', 'VBZ', 'IN' in sentence.\n",
    "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['PRP']]+ ' '+ line.words[bucket['VBG']] + '?'\n",
    "\n",
    "    \n",
    "    elif all(key in  bucket for key in l4): #'PRP', 'VBG', 'VBZ' in sentence.\n",
    "        question = 'What ' + line.words[bucket['PRP']] +' '+  ' does ' + line.words[bucket['VBG']]+ ' '+  line.words[bucket['VBG']] + '?'\n",
    "\n",
    "    elif all(key in  bucket for key in l7): #'NN', 'VBG', 'VBZ' in sentence.\n",
    "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NN']] +' '+ line.words[bucket['VBG']] + '?'\n",
    "\n",
    "    elif all(key in bucket for key in l8): #'NNP', 'VBZ', 'JJ' in sentence.\n",
    "        question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n",
    "\n",
    "    elif all(key in bucket for key in l9): #'NNP', 'VBZ', 'NN' in sentence\n",
    "        question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n",
    "\n",
    "    elif all(key in bucket for key in l11): #'PRP', 'VBZ' in sentence.\n",
    "        if line.words[bucket['PRP']] in ['she','he']:\n",
    "            question = 'What' + ' does ' + line.words[bucket['PRP']].lower() + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n",
    "\n",
    "    elif all(key in bucket for key in l10): #'NNP', 'VBZ' in sentence.\n",
    "        question = 'What' + ' does ' + line.words[bucket['NNP']] + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n",
    "\n",
    "    elif all(key in bucket for key in l13): #'NN', 'VBZ' in sentence.\n",
    "        question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NN']] + '?'\n",
    "\n",
    "    # When the tags are generated 's is split to ' and s. To overcome this issue.\n",
    "    if 'VBZ' in bucket and line.words[bucket['VBZ']] == \"’\":\n",
    "        question = question.replace(\" ’ \",\"'s \")\n",
    "\n",
    "    # Print the genetated questions as output.\n",
    "    if question != '':\n",
    "        print('\\n', 'Question: ' + question )\n",
    "   \n",
    "def main():  \n",
    "    \"\"\"\n",
    "    Accepts a text file as an argument and generates questions from it.\n",
    "    \"\"\"\n",
    "    #verbose mode is activated when we give -v as argument.\n",
    "    global verbose \n",
    "    verbose = False\n",
    "\n",
    "    # Set verbose if -v option is given as argument.\n",
    "    if len(sys.argv) >= 3: \n",
    "        if sys.argv[2] == '-v':\n",
    "            print('Verbose Mode Activated\\n')\n",
    "            verbose = True\n",
    "\n",
    "    # Open the file given as argument in read-only mode.\n",
    "try:\n",
    "    filehandle = open(sys.argv[1], 'r')\n",
    "    filehandle = open(\"testfile.txt\", \"r\") \n",
    "    textinput = filehandle.read()\n",
    "    print('\\n-----------INPUT TEXT-------------\\n')\n",
    "    print(textinput,'\\n')\n",
    "    print('\\n-----------INPUT END---------------\\n')\n",
    "\n",
    "except FileNotFoundError:\n",
    "\tprint(f\"Sorry, the file {filehandle} does not exist.\")\n",
    "    \n",
    "    # Send the content of text file as string to function parse()\n",
    "parse(textinput)\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data mining is also called as Knowledge discovery, Knowledge extraction, data/pattern analysis, information harvesting, etc.. Data mining is looking for hidden, valid, and potentially useful patterns in huge data sets. Data Mining is all about discovering unsuspected/ previously unknown relationships amongst the data.\\n\\nIt is a multi-disciplinary skill that uses machine learning, statistics, AI and database technology.\\n\\nThe insights derived via Data Mining can be used for marketing, fraud detection, and scientific discovery, etc.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"testfile.txt\", \"r\") \n",
    "file.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from textblob import Word\n",
    "import sys\n",
    "text = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n",
    "#print(nltk.pos_tag(text))\n",
    "\n",
    "   # Create a list of tag-combination\n",
    "\n",
    "l1 = ['DT', 'JJ', 'NN', 'IN']\n",
    "l2 = ['NNP', 'VBG', 'VBZ']\n",
    "    \n",
    "\n",
    "l3 = ['PRP', 'VBG', 'VBZ', 'IN']\n",
    "l4 = ['PRP', 'VBG', 'VBZ']\n",
    "l5 = ['PRP', 'VBG', 'VBD']\n",
    "l6 = ['NNP', 'VBG', 'VBD']\n",
    "l7 = ['NN', 'VBG', 'VBZ']\n",
    "\n",
    "\n",
    "tagged=nltk.pos_tag(text)\n",
    "length=len(tagged)\n",
    "print(length)\n",
    "#print (tagged)\n",
    "print(l1)\n",
    "\n",
    "#if type(line) is str:     # If the passed variable is of type string.\n",
    "\n",
    "#line = TextBlob(text) # Create object of type textblob.blob.TextBlob\n",
    "\n",
    "bucket = {}               # Create an empty dictionary\n",
    "\n",
    "print(\"Line.............\")\n",
    "#print(line)\n",
    "    \n",
    "#for i,j in enumerate(line.tags):  # line.tags are the parts-of-speach in English\n",
    "#        if j[1] not in bucket:\n",
    "#            bucket[j[1]] = i \n",
    "\n",
    "for x in range(0,length):\n",
    "    #if (\"DT\" in tagged[x]): #'NNP', 'VBG', 'VBZ', 'IN' in sentence.\n",
    "    print (tagged[x].tags)\n",
    "    if (tagged[x] in l1):\n",
    "        print (tagged[x].tags)\n",
    "        print(l1)\n",
    "        ques = \"What\" + \" \" + str(tagged[x])\n",
    "        print (ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "DT\n",
      "quick\n",
      "JJ\n",
      "brown\n",
      "NN\n",
      "fox\n",
      "NN\n",
      "jumps\n",
      "VB\n",
      "over\n",
      "IN\n",
      "the\n",
      "DT\n",
      "lazy\n",
      "JJ\n",
      "What ['brown', 'fox']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "length = len(tagged) - 1\n",
    "\n",
    "a = list()\n",
    "\n",
    "for i in range(0, length):\n",
    "    print (tagged [i][0])\n",
    "    print (tagged [i][1][0]+tagged [i][1][1])\n",
    "    log = (tagged [i][1][0] == 'N')\n",
    "    if log == True:\n",
    "        a.append(tagged [i][0])\n",
    "        ques = \"What\" + \" \" + str(a)\n",
    "print (ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  and/CC\n",
      "  (PERSON John/NNP)\n",
      "  are/VBP\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  ./.)\n",
      "NE............\n",
      "Mark\n",
      "a\n",
      "John\n",
      "a\n",
      "w\n",
      "(S\n",
      "  I/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (NE New/NNP York/NNP)\n",
      "  to/TO\n",
      "  meet/VB\n",
      "  (NE John/NNP Smith/NNP))\n",
      "Mark NNP\n",
      "and CC\n",
      "John NNP\n",
      "are VBP\n",
      "working VBG\n",
      "at IN\n",
      "Google NNP\n",
      ". .\n",
      "What ['Mark', 'John', 'Google']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"Mark and John are working at Google.\"\n",
    "\n",
    "#sent1=nltk.sent_tokenize(sentence)\n",
    "#print (sent1)\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "length = len(tagged) \n",
    "\n",
    "NE= nltk.ne_chunk(tagged)\n",
    "\n",
    "print(NE)\n",
    "      \n",
    "\n",
    "print(\"NE............\")\n",
    "\n",
    "print(NE[0][0][0])\n",
    "print(NE[1][0][0])\n",
    "print(NE[2][0][0])\n",
    "print(NE[3][0][0])\n",
    "print(NE[4][0][0])\n",
    "\n",
    "sentence = \"I went to New York to meet John Smith\";\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print (nltk.ne_chunk(pos_tags, binary=True))\n",
    "\n",
    "#print(nltk.ne_chunk(tagged[1][0]))\n",
    "\n",
    "a = list()\n",
    "\n",
    "'''\n",
    "Rule1 = [\"MOD\", \"NP\", \"V\"]\n",
    "Ans1=\n",
    "Rule2 = [\"MOD\", \"NP\", \"V\"]\n",
    "\n",
    "if \"banana\" in fruits:\n",
    "  print(\"yes\")\n",
    "\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "\n",
    "for x in fruits:\n",
    "  print(x)\n",
    "\n",
    "Wh + modal verb + subject + verb\n",
    "\n",
    "What + aux (be,do and have) + subject+ main verb lower Arg + do + ArgM ADV + ?\n",
    "    lower Arg + V + higher Arg + higher Args\n",
    "Who + V + higher Arg + ArgM ADV + ?\n",
    "    lower Arg + V + higher Arg + higher Args\n",
    "What is it that + lower Arg + V + ArgM ADV + ?\n",
    "    lower Arg + V + higher Arg + higher Args\n",
    "What happens to + lower Arg + ArgM ADV + ?\n",
    "    lower Arg + V + higher Arg + higher Args\n",
    "When + aux + lower Arg + V + higher Arg + higher Args + ?\n",
    "    ArgM ADV\n",
    "'''\n",
    "\n",
    "for i in range(0, length):\n",
    "    print (tagged [i][0]+\" \"+ tagged [i][1])\n",
    "    #print (tagged [i][1][0]+tagged [i][1][1])\n",
    "    log = (tagged [i][1][0] == 'N')\n",
    "    if log == True:\n",
    "        a.append(tagged [i][0])\n",
    "        ques = \"What\" + \" \" + str(a)\n",
    "print (ques)\n",
    "\n",
    "\n",
    "# With the use of conditional statements\n",
    "    \n",
    "#if all(key in  bucket for key in l1): #'NNP', 'VBG', 'VBZ', 'IN' in sentence.\n",
    "    #question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']]+ ' '+ line.words[bucket['VBG']] + '?'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S What/WP weather/NN ./. (GPE Chicago/NNP) today/NN ?/.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['New York', 'Chicago']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import load_parser\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = \"What is the weather in Chicago today? \"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "clean_tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "tagged = nltk.pos_tag(clean_tokens)\n",
    "\n",
    "print(nltk.ne_chunk(tagged))\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import Tree\n",
    "\n",
    "def get_continuous_chunks(text, label):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == Tree and subtree.label() == label:\n",
    "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return continuous_chunk\n",
    "\n",
    "sent = \"What is the weather in New York today?\"\n",
    "get_continuous_chunks(sent, 'GPE')\n",
    "\n",
    "sent = \"What is the weather in New York and Chicago today?\"\n",
    "get_continuous_chunks(sent, 'GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "apple\n",
      "banana\n",
      "cherry\n",
      "geeks\n",
      "1-2-3-4\n",
      "[2, 1, 1, 4, 5]\n",
      "['b', 'c', 'd']\n",
      "[2, 3, 4, 4, 5]\n",
      "[65, 39, 221]\n",
      "[5, 7, 97, 77, 23, 73, 61]\n",
      "Tree Example\n",
      "(S (NP I) (VP (V enjoyed) (NP my cookie)))\n",
      "2nd key using loop : for\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8135c6bb89bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;31m# accessing 2nd element using keys()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'2nd key using keys() : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "\n",
    "if \"banana\" in fruits:\n",
    "  print(\"yes\")\n",
    "\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "\n",
    "for x in fruits:\n",
    "  print(x)\n",
    "\n",
    "list1 = ['g','e','e','k', 's']  \n",
    "print(\"\".join(list1)) \n",
    "\n",
    "\n",
    "list1 = ['1','2','3','4']  \n",
    "  \n",
    "s = \"-\"\n",
    "s = s.join(list1) \n",
    "  \n",
    "print(s) \n",
    "\n",
    "# the first occurrence of 1 is removed from the list  \n",
    "list1 = [ 1, 2, 1, 1, 4, 5 ]  \n",
    "list1.remove(1)  \n",
    "print(list1)  \n",
    "  \n",
    "# removes 'a' from list2  \n",
    "list2 = [ 'a', 'b', 'c', 'd' ]  \n",
    "list2.remove('a')  \n",
    "print(list2)\n",
    "\n",
    "\n",
    "# Python3 program for practical application \n",
    "# of removing 1 untill all 1 are removed from the list \n",
    "\n",
    "list1 = [1, 2, 3, 4, 1, 1, 1, 4, 5] \n",
    "\n",
    "# looping till all 1's are removed \n",
    "while (list1.count(1)): \n",
    "\tlist1.remove(1) \n",
    "\t\n",
    "print(list1) \n",
    "\n",
    "\n",
    "# Python Program to find numbers divisible \n",
    "# by thirteen from a list using anonymous \n",
    "# function \n",
    "\n",
    "# Take a list of numbers. \n",
    "my_list = [12, 65, 54, 39, 102, 339, 221, 50, 70, ] \n",
    "\n",
    "# use anonymous function to filter and comparing \n",
    "# if divisible or not \n",
    "result = list(filter(lambda x: (x % 13 == 0), my_list)) \n",
    "\n",
    "# printing the result \n",
    "print(result) \n",
    "\n",
    "\n",
    "# Python code to illustrate \n",
    "# filter() with lambda() \n",
    "li = [5, 7, 22, 97, 54, 62, 77, 23, 73, 61] \n",
    "final_list = list(filter(lambda x: (x%2 != 0) , li)) \n",
    "print(final_list) \n",
    "\n",
    "print(\"Tree Example\")\n",
    "from nltk.tree import Tree\n",
    "tree = Tree.fromstring('(S (NP I) (VP (V enjoyed) (NP my cookie)))')\n",
    "print(tree)\n",
    "\n",
    "\n",
    "# Python program to demonstrate \n",
    "# working of keys() \n",
    "\n",
    "# initializing dictionary \n",
    "test_dict = { \"geeks\" : 7, \"for\" : 1, \"geeks\" : 2 } \n",
    "\n",
    "# accessing 2nd element using naive method \n",
    "# using loop \n",
    "j = 0\n",
    "for i in test_dict: \n",
    "\tif(j==1): \n",
    "\t\tprint ('2nd key using loop : ' + i) \n",
    "\tj = j + 1\n",
    "\n",
    "# accessing 2nd element using keys() \n",
    "print ('2nd key using keys() : ' + test_dict.keys()[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the ./stanford-ner-tagger/ner-model-english.ser.gz file!\nUse software specific configuration paramaters or set the STANFORD_MODELS environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-00b7da949982>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Prepare NER tagger with english model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mner_tagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Tokenize: Split sentence into words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         self._stanford_model = find_file(\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mmodel_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'STANFORD_MODELS'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         )\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose)\u001b[0m\n\u001b[0;32m    644\u001b[0m ):\n\u001b[0;32m    645\u001b[0m     return next(\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mfind_file_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m     )\n\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n\\n  For more information on %s, see:\\n    <%s>'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the ./stanford-ner-tagger/ner-model-english.ser.gz file!\nUse software specific configuration paramaters or set the STANFORD_MODELS environment variable.\n==========================================================================="
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "sentence = u\"Twenty miles east of Reno, Nev., \" \\\n",
    "    \"where packs of wild mustangs roam free through \" \\\n",
    "    \"the parched landscape, Tesla Gigafactory 1 \" \\\n",
    "    \"sprawls near Interstate 80.\"\n",
    "\n",
    "jar = './stanford-ner-tagger/stanford-ner.jar'\n",
    "model = './stanford-ner-tagger/ner-model-english.ser.gz'\n",
    "\n",
    "# Prepare NER tagger with english model\n",
    "ner_tagger = StanfordNERTagger(model, jar, encoding='utf8')\n",
    "\n",
    "# Tokenize: Split sentence into words\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Run NER tagger on words\n",
    "print(ner_tagger.tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'method' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ab51e9f3f96c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtreebank\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#Auxiliary + subject + main verb ( + extra information/details/adverbs, etc…)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m '''\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\named_entity.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtagged\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \"\"\"\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tagged_to_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# docs inherited from TaggerI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'method' has no len()"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "length = len(tagged)\n",
    "\n",
    "#sent = nltk.corpus.treebank.tagged_sents\n",
    "print (nltk.ne_chunk(sent))\n",
    "#Auxiliary + subject + main verb ( + extra information/details/adverbs, etc…)\n",
    "\n",
    "'''\n",
    "Wh + modal verb + subject + verb / What+MOD+NP+V+?\n",
    "\n",
    "What + aux (be,do and have) + subject+ main verb lower Arg + do + ArgM ADV + ?/\n",
    "    lower Arg + V + higher Arg + higher Args\n",
    "\n",
    "Who + V + higher Arg + ArgM ADV + ?\n",
    "    lower Arg + V + higher Arg + higher Args\n",
    "What is it that + lower Arg + V + ArgM ADV + ?\n",
    "    lower Arg + V + higher Arg + higher Args\n",
    "What happens to + lower Arg + ArgM ADV + ?\n",
    "    lower Arg + V + higher Arg + higher Args\n",
    "When + aux + lower Arg + V + higher Arg + higher Args + ?\n",
    "    ArgM ADV\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, length):\n",
    "    print (tagged [i][0]+\" \"+ tagged [i][1])\n",
    "    #log = (tagged [i][1][0] == 'N')\n",
    "    Q=list()\n",
    "    \n",
    "    if ((tagged [i][1]==\"MOD\") and (tagged [i+1][1]==\"NP\") and (tagged [i+2][1]==\"V\") and (tagged [i+3][1]==\"NN\")):\n",
    "        q1 = \"What\" + \" is \" + tagged [i+1][0]\n",
    "        print (q1)\n",
    "    elif tagged [i+1][1]==\"NN\":\n",
    "        q1=\"What\" + \" is \" + tagged [i+1][0]+tagged [i+2][0]+\"?\"\n",
    "        #Q.append(q1)\n",
    "        print (q1)\n",
    "    else:\n",
    "        print (Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent\n",
      "[('This', 'DT'), ('is', 'VBZ'), ('possible', 'JJ'), ('as', 'IN'), ('his', 'PRP$'), ('party', 'NN'), ('defeated', 'VBD'), ('the', 'DT'), ('Liberals', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('Provincial', 'NNP'), ('elections', 'NNS'), ('held', 'VBD'), ('on', 'IN'), ('October', 'NNP'), ('1st', 'CD'), ('2018', 'CD'), ('.', '.'), ('European', 'JJ'), ('authorities', 'NNS'), ('fined', 'VBD'), ('Google', 'NNP'), ('a', 'DT'), ('record', 'NN'), ('$', '$'), ('5.1', 'CD'), ('billion', 'CD'), ('on', 'IN'), ('Wednesday', 'NNP'), ('for', 'IN'), ('abusing', 'VBG'), ('its', 'PRP$'), ('power', 'NN'), ('in', 'IN'), ('the', 'DT'), ('mobile', 'JJ'), ('phone', 'NN'), ('market', 'NN'), ('and', 'CC'), ('ordered', 'VBD'), ('the', 'DT'), ('company', 'NN'), ('to', 'TO'), ('alter', 'VB'), ('its', 'PRP$'), ('practices', 'NNS')]\n",
      "ne.nltk\n",
      "(S\n",
      "  This/DT\n",
      "  is/VBZ\n",
      "  possible/JJ\n",
      "  as/IN\n",
      "  his/PRP$\n",
      "  party/NN\n",
      "  defeated/VBD\n",
      "  the/DT\n",
      "  (ORGANIZATION Liberals/NNS)\n",
      "  in/IN\n",
      "  the/DT\n",
      "  Provincial/NNP\n",
      "  elections/NNS\n",
      "  held/VBD\n",
      "  on/IN\n",
      "  October/NNP\n",
      "  1st/CD\n",
      "  2018/CD\n",
      "  ./.\n",
      "  (GPE European/JJ)\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  (PERSON Google/NNP)\n",
      "  a/DT\n",
      "  record/NN\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  power/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  mobile/JJ\n",
      "  phone/NN\n",
      "  market/NN\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  the/DT\n",
      "  company/NN\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sent1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2f55353011ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0msent1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;31m#print(sent1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sent1' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "'''\n",
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "print(\"Sent\")\n",
    "print(sent)\n",
    "'''\n",
    "sent = \"This is possible as his party defeated the Liberals in the Provincial elections held on October 1st 2018. European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices\"\n",
    "sent = nltk.word_tokenize(sent)\n",
    "sent = nltk.pos_tag(sent)\n",
    "print(\"Sent\")\n",
    "print(sent)\n",
    "print(\"ne.nltk\")\n",
    "print(nltk.ne_chunk(sent))\n",
    "\n",
    "\n",
    "    \n",
    "z = \"This is possible as his party defeated the Liberals in the Provincial elections held on October 1st 2018. European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices\"\n",
    "\n",
    "def preprocess_pipe1(doc1):\n",
    "    sent1 = nltk.sent_tokenize(doc1)\n",
    "#print(sent1)\n",
    "    print(\" \")\n",
    "    print (\"SENTENCE SPLITTER\")\n",
    "    for x in sent1:\n",
    "        print(x)\n",
    "\n",
    "    print(\" \")\n",
    "\n",
    "sent1 = [nltk.word_tokenize(sent2) for sent2 in sent1]\n",
    "#print(sent1)\n",
    "print(\" \")\n",
    "print (\"TOKENIZER\")\n",
    "for x in sent1:\n",
    "    print(x)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "sent1 = [nltk.pos_tag(sent2) for sent2 in sent1]\n",
    "#print(sent1)\n",
    "print(\" \")\n",
    "print (\"POS TAGGER\")\n",
    "for x in sent1:\n",
    "    print(x)\n",
    "\n",
    "return(sent1)\n",
    "\n",
    "\n",
    "sent2=preprocess_pipe1(sent)\n",
    "sent3=nltk.ne_chunk(sent2)\n",
    "print(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk.RegexpParser with 5 stages:\n",
      "RegexpChunkParser with 3 rules:\n",
      "       <ChunkRule: '<NNP>+'>\n",
      "       <ChunkRule: '<NN>+'>\n",
      "       <ChunkRule: '<NNP><NN>'>\n",
      "RegexpChunkParser with 2 rules:\n",
      "       <ChunkRule: '<PERSON>'>\n",
      "       <ChunkRule: '<PERSON><ORGANISATION>'>\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<ORGANIZATION><NP>'>\n",
      "RegexpChunkParser with 5 rules:\n",
      "       <ChunkRule: '<GPE><ORGANIZATION>'>\n",
      "       <ChunkRule: '<ORGANIZATION>'>\n",
      "       <ChunkRule: '<GPE><NP>'>\n",
      "       <ChunkRule: '<GPE>'>\n",
      "       <ChunkRule: '<JJ><NNP><NNP>'>\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<VB|VBD|VBG|VBN|VBP|VBZ>'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Tree() argument 2 should be a list, not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m             \u001b[0mchunk_struct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'label'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8b42f44aed22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Hello world\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mregexChunker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mregexChunker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     grammar = \"\"\"NP:{<NNP>+}\n\u001b[0;32m      5\u001b[0m                     \u001b[1;33m{\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-5d0994bac158>\u001b[0m in \u001b[0;36mregexChunker\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mchunked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegexpParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[0;32m   1290\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m                 \u001b[0mchunk_struct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_struct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[0;32m   1090\u001b[0m             \u001b[0mchunk_struct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mchunk_struct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;31m# Use the default trace value?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node, children)\u001b[0m\n\u001b[0;32m    108\u001b[0m             raise TypeError(\n\u001b[0;32m    109\u001b[0m                 \u001b[1;34m\"%s() argument 2 should be a list, not a \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[1;34m\"string\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             )\n\u001b[0;32m    112\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Tree() argument 2 should be a list, not a string"
     ]
    }
   ],
   "source": [
    "sentence=\"Hello world\"\n",
    "regexChunker(sentence)\n",
    "def regexChunker(sentence):\n",
    "    grammar = \"\"\"NP:{<NNP>+}\n",
    "                    {<NN>+}\n",
    "                    {<NNP><NN>}\n",
    "                 PERSON:{<PERSON>}\n",
    "                      {<PERSON><ORGANISATION>}\n",
    "                 PLACE: {<ORGANIZATION><NP>}\n",
    "                 ORG:{<GPE><ORGANIZATION>}\n",
    "                     {<ORGANIZATION>}\n",
    "                     {<GPE><NP>}\n",
    "                     {<GPE>}\n",
    "                     {<JJ><NNP><NNP>}\n",
    "                ACTION:{<VB|VBD|VBG|VBN|VBP|VBZ>}\n",
    "\"\"\"\n",
    "    chunked = nltk.RegexpParser(grammar)\n",
    "    print (chunked)\n",
    "    return chunked.parse(sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "green\n",
      "skirt\n"
     ]
    }
   ],
   "source": [
    "from nltk import Tree\n",
    "\n",
    "chunk_output = Tree('S', [Tree('NP', [('This', 'DT'), ('beautiful', 'JJ'), ('green','JJ'), ('skirt', 'NN')]), ('is', 'VBZ'), ('for', 'IN'), ('you', 'PRP'), ('.', '.')])\n",
    "\n",
    "for child in chunk_output:\n",
    "    if isinstance(child, Tree):               \n",
    "        if child.label() == 'NP':\n",
    "            for num in range(len(child)):\n",
    "                if not (child[num][1]=='JJ' and child[num+1][1]=='JJ'):\n",
    "                    print (child[num][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "y\n",
      "\n",
      "Default download directory: C:\\Users\\Walelign\\stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: C:\\Users\\Walelign\\stanfordnlp_resources\\en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235M/235M [00:42<00:00, 5.49MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: C:\\Users\\Walelign\\stanfordnlp_resources\\en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Walelign\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Walelign\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\Walelign\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Walelign\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Walelign\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\Walelign\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "nlp = stanfordnlp.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk import ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "    print nltk.ne_chunk(sent) \n",
    "sent = preprocess(ex)\n",
    "sent\n",
    "\n",
    "\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "'''\n",
    "print(cs)\n",
    "print (\"\")\n",
    "'''\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)\n",
    "\n",
    "ne_tree = ne_chunk(pos_tag(word_tokenize(ex)))\n",
    "print(ne_tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "pprint([(X.text, X.label_) for X in doc.ents])\n",
    "\n",
    "\n",
    "pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\walelign\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (41.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (1.16.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.36.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (0.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\walelign\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (7.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-dde1424929d0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-43-dde1424929d0>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    python -m spacy download en\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "python -m spacy download en\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import time\n",
    "\n",
    "exampleArray = ['The incredibly intimidating NLP scares people away who are sissies.']\n",
    "\n",
    "\n",
    "contentArray =['Starbucks is not doing very well lately.',\n",
    "               'Overall, while it may seem there is already a Starbucks on every corner, Starbucks still has a lot of room to grow.',\n",
    "               'They just began expansion into food products, which has been going quite well so far for them.',\n",
    "               'I can attest that my own expenditure when going to Starbucks has increased, in lieu of these food products.',\n",
    "               'Starbucks is also indeed expanding their number of stores as well.',\n",
    "               'Starbucks still sees strong sales growth here in the united states, and intends to actually continue increasing this.',\n",
    "               'Starbucks also has one of the more successful loyalty programs, which accounts for 30%  of all transactions being loyalty-program-based.',\n",
    "               'As if news could not get any more positive for the company, Brazilian weather has become ideal for producing coffee beans.',\n",
    "               'Brazil is the world\\'s #1 coffee producer, the source of about 1/3rd of the entire world\\'s supply!',\n",
    "               'Given the dry weather, coffee farmers have amped up production, to take as much of an advantage as possible with the dry weather.',\n",
    "               'Increase in supply... well you know the rules...',]\n",
    "\n",
    "\n",
    "\n",
    "##let the fun begin!##\n",
    "def processLanguage():\n",
    "    try:\n",
    "        for item in contentArray:\n",
    "            tokenized = nltk.word_tokenize(item)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            print (tagged)\n",
    "\n",
    "            namedEnt = nltk.ne_chunk(tagged)\n",
    "            namedEnt.draw()\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    except Exception:\n",
    "          print (\"Error\")\n",
    "    processLanguage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A sample code usage of the python package stanfordcorenlp to access a Stanford CoreNLP server.\n",
    "Written as part of the blog post: https://www.khalidalnajjar.com/how-to-setup-and-use-stanford-corenlp-server-with-python/ \n",
    "'''\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import logging\n",
    "import json\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sNLP = StanfordNLP()\n",
    "    text = 'A blog post using Stanford CoreNLP Server. Visit www.khalidalnajjar.com for more details.'\n",
    "    print (\"Annotate:\", sNLP.annotate(text))\n",
    "    print (\"POS:\", sNLP.pos(text))\n",
    "    print (\"Tokens:\", sNLP.word_tokenize(text))\n",
    "    print (\"NER:\", sNLP.ner(text))\n",
    "    print (\"Parse:\", sNLP.parse(text))\n",
    "    print (\"Dep Parse:\", sNLP.dependency_parse(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing list    and convert into set object\n",
    "n = set(['n1','n4','n3','n2'])\n",
    "\n",
    "#Add new Element in set n\n",
    "n.add('n5');\n",
    "\n",
    "print(\"Output with set Function : \")\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to demonstrate \n",
    "# checking of element existence \n",
    "# using loops and in \n",
    "\n",
    "# Initializing list \n",
    "test_list = [ 1, 6, 3, 5, 3, 4 ] \n",
    "\n",
    "print(\"Checking if 4 exists in list ( using in ) : \") \n",
    "\n",
    "# Checking if 4 exists in list \n",
    "# using in \n",
    "if (4 in test_list): \n",
    "\tprint (\"Element Exists\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
